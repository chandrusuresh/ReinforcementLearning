{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy Iteration.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP9lmb5oAsnqXzxrY8/t+E+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandrusuresh/ReinforcementLearning/blob/master/Policy_Iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygpKuMI3tYoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.special import factorial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGu08hpiAB1n",
        "colab_type": "text"
      },
      "source": [
        "##Policy Iteration\n",
        "Policy iteration is an algorithm for evaluating the value function and improving it in successive iterations. Each iteration involves 2 steps - first step evaluates the policy and the second improves it. The first step is the same as the iterative policy evaluation algorithm and second step is the iteration policy improvement algorithm. Refer to the [Grid World notebook](https://github.com/chandrusuresh/ReinforcementLearning/blob/master/GridWorld.ipynb) for more details on these two steps/algorithms.\n",
        "\n",
        "The state and action value function equations are summarized here:\n",
        "\n",
        "$$\\textbf{State Value Function:  }  v_{k+1}(s) = \\sum_a{\\pi(a|s)} \\sum_{a,s'}{p(s',r|a,s) \\left(r + \\gamma v_k (s')\\right)}  $$\n",
        "\n",
        "$$\\textbf{Action Value Function:  }  q_\\pi (s,a) = \\sum_{r,s'}{p(s',r|a,s) \\left(r + \\gamma \\sum_{a'}{\\pi(a'|s') q_\\pi (s',a')}\\right)}  $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSMDJU8-AGUK",
        "colab_type": "text"
      },
      "source": [
        "## Jack's Rental Car Problem\n",
        "\n",
        "Jack manages two locations for a nationwide car\n",
        "rental company. Each day, some number of customers arrive at each location to rent cars.\n",
        "\n",
        "Cost of rental = \\$10\n",
        "\n",
        "Cost of moving cars = -$2\n",
        "\n",
        "Max Cars = 20\n",
        "\n",
        "Max cars moved = 5\n",
        "\n",
        "Number of cars rented/returned is a Poisson distribution given by,\n",
        "$$P(rented/returned = n) = \\frac{\\lambda^n}{n!} e^{-\\lambda}$$ where $\\lambda$ is the expected number.\n",
        "\n",
        "$\\lambda_{rent}^1 = 3$, $\\lambda_{rent}^2 = 4$ and\n",
        "\n",
        "$\\lambda_{return}^1 = 3$,$\\lambda_{return}^2 = 2$\n",
        "\n",
        "$gamma = 0.9$\n",
        "Time steps = days\n",
        "\n",
        "State variable = Number of cars at each location at the end of the day\n",
        "\n",
        "Action variable = Number of cars moved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHCm4eFmSgT7",
        "colab_type": "text"
      },
      "source": [
        "## Solution\n",
        "The state vector, $s_t = \\left[\\begin{matrix}s_1 \\\\ s_2 \\end{matrix}\\right]$ where $s_i$ is the number of cars in location $i$ at the beginning of day $t$\n",
        "\n",
        "The action vector, $a_t$ is the number of cars moved between locations 1 & 2 at the end of day $t$\n",
        "\n",
        "The transition probability, $p_{trans} = p(r,s_{t+1}|s_t,a_t)$\n",
        "\n",
        "Assuming the rentals & returns are independent of each other and location, we have:\n",
        "\n",
        "$$\\begin{align*} p_{trans,i} &= p(r,s_{t+1,i}|s_{t,i},a_t) \\\\ \n",
        "&= p(rent=x,retn=y|s_{t,i},a_t) \\\\\n",
        " &= p(retn=y|rent=x)\\times p(rent=x) \\end{align*}$$ where $s_{t+1} = min(20,max(0,s_t-x+y)) $\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9XXuP29AFjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_cars = 20\n",
        "max_move = 5\n",
        "rent = [3,4]\n",
        "retn = [3,2]\n",
        "rent_cost = 10\n",
        "move_cost = -2\n",
        "gamma = 0.9\n",
        "\n",
        "def poisson(lamda,n):\n",
        "  return lamda**n/factorial(n)*np.exp(-lamda)\n",
        "\n",
        "def prob_rent(init_cars,lambda_rent):\n",
        "  sum_prob = 0\n",
        "  prob_rent = dict()\n",
        "  for i in range(init_cars):\n",
        "    prob_rent[i] = poisson(lambda_rent,i)\n",
        "    sum_prob = sum_prob + prob_rent[i]\n",
        "  prob_rent[init_cars] = 1 - sum_prob\n",
        "  return prob_rent\n",
        "\n",
        "def prob_retn(init,prob_rent,lambda_retn):\n",
        "  prob_cars = dict()\n",
        "  for k in prob_rent.keys():\n",
        "    c = init-k\n",
        "    sum_prob = 0\n",
        "    for i in range(max_cars-c):\n",
        "      prob = poisson(lambda_retn,i)\n",
        "      sum_prob = sum_prob + prob\n",
        "      if c+i in prob_cars.keys():\n",
        "        prob_cars[c+i] = prob_cars[c+i] + prob_rent[k]*prob\n",
        "      else:\n",
        "        prob_cars[c+i] = prob_rent[k]*prob\n",
        "    if max_cars in prob_cars.keys():\n",
        "      prob_cars[max_cars] = prob_cars[max_cars] + prob_rent[k]*(1-sum_prob)\n",
        "    else:\n",
        "      prob_cars[max_cars] = prob_rent[k]*(1-sum_prob)\n",
        "  return prob_cars\n",
        "\n",
        "def transition(lambda_rent,lambda_retn):\n",
        "  p_trans = dict()\n",
        "  for init_cars in range(max_cars+1):\n",
        "    p_rent = prob_rent(init_cars,lambda_rent)\n",
        "    p_trans[init_cars] = prob_retn(init_cars,p_rent,lambda_retn)\n",
        "  return p_trans\n",
        "\n",
        "p_trans_1 = transition(rent[0],retn[0])\n",
        "p_trans_2 = transition(rent[1],retn[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY00p4RwXIH-",
        "colab_type": "text"
      },
      "source": [
        "### Initialize State Value Fuction and Policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x-hvWVIXHMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_value = np.zeros((max_cars+1,max_cars+1))\n",
        "actions = list(range(-max_move,max_move+1))\n",
        "# pi = np.zeros((2*max_move+1,max_cars+1,max_cars+1))\n",
        "# for j in range(pi.shape[2]):\n",
        "#   for i in range(pi.shape[1]):\n",
        "#     for k in range(max_move+1):\n",
        "#       if k == 0:\n",
        "#         pi[max_move,i,j] = 1\n",
        "#       else:\n",
        "#         if k <= i:\n",
        "#           pi[max_move+k,i,j] = 1\n",
        "#         if k <= j:\n",
        "#           pi[max_move-k,i,j] = 1\n",
        "# ## Normalize pi\n",
        "# for i in range(pi.shape[1]):\n",
        "#   for j in range(pi.shape[2]):\n",
        "#     pi[:,i,j] = pi[:,i,j]/np.sum(pi[:,i,j])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wec_RV85RH_p",
        "colab_type": "text"
      },
      "source": [
        "### Policy Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbzEXuiSXhPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(v_s,pi):\n",
        "  vs_new = np.copy(v_s)\n",
        "  for i in range(max_cars+1):\n",
        "    for j in range(max_cars+1):\n",
        "      vs = state_value[i,j]\n",
        "      val_sum = 0\n",
        "      for a in range(pi.shape[0]):\n",
        "        if pi[a,i,j] == 0:\n",
        "          continue\n",
        "        # new_i = i\n",
        "        # new_j = j\n",
        "        new_i = min(max_cars,i - actions[a])\n",
        "        new_j = min(max_cars,j + actions[a])\n",
        "        reward = move_cost*np.abs(actions[a])\n",
        "        trans_prob_sum = 0\n",
        "        for ii in range(max_cars+1):\n",
        "          p_ii = p_trans_1[new_i][ii]\n",
        "          for jj in range(max_cars+1):\n",
        "            p_jj = p_trans_2[new_j][jj]\n",
        "            prob = p_ii*p_jj\n",
        "            trans_prob_sum = trans_prob_sum + prob*(reward+gamma*v_s[ii,jj])\n",
        "        val_sum = val_sum + pi[a,i,j]*trans_prob_sum\n",
        "      vs_new[i,j] = val_sum\n",
        "  return vs_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPCaRuGUweGI",
        "colab_type": "text"
      },
      "source": [
        "### Policy Improvement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tBsFoFhwgeO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df3ab7f7-13a3-47e0-9139-79f24ceec740"
      },
      "source": [
        "def improve_policy(v_s):\n",
        "  pi_new = np.zeros((len(actions),v_s.shape[0],v_s.shape[1]))\n",
        "  for i in range(v_s.shape[0]):\n",
        "    for j in range(v_s.shape[1]):\n",
        "      val = dict()\n",
        "      for a in range(len(actions)):\n",
        "        new_i = min(max_cars,i - actions[a])\n",
        "        new_j = min(max_cars,j + actions[a])\n",
        "        if new_i >= 0 and new_j >= 0:\n",
        "          if v_s[new_i,new_j] in val.keys():\n",
        "            val[v_s[new_i,new_j]].append(a)\n",
        "          else:\n",
        "            val[v_s[new_i,new_j]] = [a]\n",
        "      # print(i,j,val)\n",
        "      max_val = np.max(list(val.keys()))\n",
        "      for a in val[max_val]:\n",
        "        pi_new[a,i,j] = 1/float(len(val[max_val]))\n",
        "  return pi_new\n",
        "new_pi = improve_policy(state_value)\n",
        "# del_pi = (new_pi- pi)[:,:,0]\n",
        "# print(np.round(del_pi,3))\n",
        "print(np.max(np.abs(new_pi- pi)))"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEHW5vv9OorU",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY6CghJuIBEU",
        "colab_type": "code",
        "outputId": "8e703501-4684-4543-9b89-db1f4a3b9290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "cars_init = [7,7]\n",
        "p_rent = prob_rent(cars_init[1],rent[1])\n",
        "p_retn = prob_retn(cars_init[1],p_rent,retn[1])\n",
        "print(\"Probability of Rent\")\n",
        "print(p_rent)\n",
        "print(np.sum(list(p_rent.values())))\n",
        "print(\"Probability of Return\")\n",
        "\n",
        "total_prob = 0\n",
        "for i in range(max_cars+1):\n",
        "  print(i,p_retn[i]-p_trans_2[cars_init[1]][i])\n",
        "  total_prob = total_prob + p_retn[i]\n",
        "print(total_prob)\n",
        "print(np.sum(list(p_retn.values())))"
      ],
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of Rent\n",
            "{0: 0.01831563888873418, 1: 0.07326255555493671, 2: 0.14652511110987343, 3: 0.19536681481316456, 4: 0.19536681481316456, 5: 0.15629345185053165, 6: 0.1041956345670211, 7: 0.11067397840257387}\n",
            "1.0\n",
            "Probability of Return\n",
            "0 0.0\n",
            "1 0.0\n",
            "2 0.0\n",
            "3 0.0\n",
            "4 0.0\n",
            "5 0.0\n",
            "6 0.0\n",
            "7 0.0\n",
            "8 0.0\n",
            "9 0.0\n",
            "10 0.0\n",
            "11 0.0\n",
            "12 0.0\n",
            "13 0.0\n",
            "14 0.0\n",
            "15 0.0\n",
            "16 0.0\n",
            "17 0.0\n",
            "18 0.0\n",
            "19 0.0\n",
            "20 0.0\n",
            "1.0000000000000002\n",
            "0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PF08qZuYrSgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1fcf2dc8-bd2b-4a7e-c256-200a12590483"
      },
      "source": [
        "# print(state_value[0,0])\n",
        "max_iters = 50\n",
        "iter = 0\n",
        "vs_prev = state_value\n",
        "error = 1E5\n",
        "convergence = []\n",
        "while iter < max_iters and error > 0.1:\n",
        "  pi = improve_policy(vs_prev)\n",
        "  vs_new = evaluate_policy(vs_prev,pi)\n",
        "  error = np.max(np.abs(vs_new-vs_prev))\n",
        "  vs_prev = vs_new\n",
        "  print(iter,error)\n",
        "  convergence.append(error)\n",
        "  iter = iter+1\n",
        "  \n",
        "print(np.round(vs_new,1))"
      ],
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5.454545454545458\n",
            "1 9.504054323621\n",
            "2 10.838609499039983\n",
            "3 10.16353286253598\n",
            "4 9.980824825570991\n",
            "5 9.055050233266016\n",
            "6 7.697964619782638\n",
            "7 7.323820124860347\n",
            "8 7.254568739264418\n",
            "9 2.807780498027654\n",
            "10 9.062613651216381\n",
            "11 7.219012546639682\n",
            "12 5.549291970007189\n",
            "13 4.427152613723962\n",
            "14 6.852099974250972\n",
            "15 0.6420663662344914\n",
            "16 0.5230006492801067\n",
            "17 0.45581487115023833\n",
            "18 0.39900456068657775\n",
            "19 0.3498105257255091\n",
            "20 4.275147274821215\n",
            "21 2.2153453043628204\n",
            "22 0.24360425284415754\n",
            "23 0.21408827118834495\n",
            "24 2.3379471481149494\n",
            "25 0.1635192581894067\n",
            "26 0.14520613292609852\n",
            "27 0.13019399765464357\n",
            "28 0.11665397331785954\n",
            "29 0.10450586583887755\n",
            "30 0.09365544963666039\n",
            "[[-21.4 -23.5 -23.5 -25.6 -25.6 -27.9 -28.  -28.3 -30.6 -28.3 -26.2 -29.9\n",
            "  -30.4 -30.7 -30.8 -30.9 -31.  -31.1 -31.1 -31.2 -31.3]\n",
            " [-21.5 -21.5 -23.6 -23.6 -25.9 -26.  -26.3 -28.6 -26.3 -28.2 -27.9 -28.4\n",
            "  -28.7 -28.8 -28.9 -29.  -29.1 -29.1 -29.2 -29.3 -29.3]\n",
            " [-23.5 -21.6 -21.6 -23.9 -24.  -24.3 -26.6 -24.3 -32.8 -25.9 -26.4 -26.7\n",
            "  -26.8 -26.9 -27.  -27.1 -27.1 -27.2 -27.3 -27.3 -27.4]\n",
            " [-23.6 -23.6 -21.9 -22.  -22.3 -24.6 -32.4 -32.4 -32.5 -32.7 -28.7 -28.8\n",
            "  -28.9 -29.  -29.1 -29.1 -29.2 -29.3 -29.3 -29.4 -29.4]\n",
            " [-25.6 -23.9 -24.  -24.3 -22.6 -31.9 -30.4 -32.  -32.1 -32.3 -30.8 -30.9\n",
            "  -31.  -31.1 -31.1 -31.2 -31.3 -31.3 -31.4 -31.4 -31.4]\n",
            " [-25.9 -26.  -26.3 -24.6 -29.9 -28.4 -30.  -31.5 -31.6 -31.9 -32.4 -33.\n",
            "  -33.1 -33.1 -33.2 -33.3 -33.3 -33.4 -33.4 -33.4 -33.4]\n",
            " [-28.  -28.3 -26.6 -27.9 -26.4 -28.  -29.5 -31.2 -31.3 -31.6 -32.1 -35.1\n",
            "  -35.1 -35.2 -35.3 -35.3 -37.2 -35.4 -35.4 -35.4 -35.4]\n",
            " [-30.3 -28.6 -25.9 -24.4 -26.  -27.5 -29.2 -30.9 -31.  -31.3 -31.9 -37.1\n",
            "  -37.2 -37.3 -37.3 -35.2 -37.4 -37.4 -37.4 -37.4 -37.4]\n",
            " [-30.6 -23.9 -22.4 -24.  -25.5 -27.2 -28.9 -30.6 -30.7 -31.1 -31.7 -32.6\n",
            "  -37.7 -37.8 -33.2 -33.6 -35.5 -34.2 -33.1 -38.2 -35.3]\n",
            " [-21.9 -24.4 -22.  -23.5 -25.2 -26.9 -28.6 -30.4 -30.6 -30.9 -31.5 -32.4\n",
            "  -33.5 -31.2 -31.6 -33.5 -32.2 -31.1 -36.2 -33.3 -32.2]\n",
            " [-26.4 -24.  -21.5 -23.2 -24.9 -26.6 -28.4 -30.3 -30.4 -30.8 -31.4 -32.3\n",
            "  -29.2 -29.6 -31.5 -30.2 -29.1 -34.2 -31.3 -30.2 -30.5]\n",
            " [-26.  -23.5 -21.2 -22.9 -24.6 -26.4 -28.3 -30.1 -30.3 -30.7 -31.3 -27.2\n",
            "  -27.6 -29.5 -28.2 -31.1 -32.2 -29.3 -37.9 -32.5 -38.5]\n",
            " [-25.5 -23.2 -20.9 -22.6 -24.4 -26.3 -28.1 -30.  -30.2 -30.6 -31.2 -29.6\n",
            "  -27.5 -30.2 -33.1 -30.2 -31.3 -37.5 -37.8 -36.5 -34.5]\n",
            " [-25.2 -22.9 -20.6 -22.4 -24.3 -26.1 -28.  -30.  -30.1 -30.5 -31.2 -29.5\n",
            "  -32.2 -35.1 -28.2 -33.3 -35.5 -37.4 -34.5 -38.  -38.3]\n",
            " [-24.9 -22.6 -20.4 -22.3 -24.1 -26.  -28.  -29.9 -30.1 -30.5 -31.2 -32.1\n",
            "  -37.1 -30.2 -35.3 -33.5 -35.4 -32.5 -37.7 -36.3 -36.7]\n",
            " [-24.6 -22.4 -20.3 -22.1 -24.  -26.  -27.9 -29.9 -30.  -30.4 -31.1 -32.1\n",
            "  -32.2 -37.3 -31.5 -33.4 -30.5 -35.7 -34.3 -34.7 -31.9]\n",
            " [-24.4 -22.3 -20.1 -22.  -24.  -25.9 -27.9 -29.9 -30.  -30.4 -31.1 -32.1\n",
            "  -39.3 -29.5 -31.4 -28.5 -33.7 -32.3 -32.7 -29.9 -30.2]\n",
            " [-24.3 -22.1 -20.  -22.  -23.9 -25.9 -27.9 -29.9 -30.  -30.4 -31.1 -32.1\n",
            "  -27.5 -29.4 -30.5 -31.7 -30.3 -30.7 -31.9 -32.2 -32.2]\n",
            " [-24.1 -22.  -20.  -21.9 -23.9 -25.9 -27.9 -29.9 -30.  -30.4 -31.1 -29.5\n",
            "  -27.4 -32.5 -29.7 -28.3 -28.7 -37.4 -34.2 -34.2 -34.2]\n",
            " [-24.  -22.  -19.9 -21.9 -23.9 -25.9 -27.9 -29.9 -30.  -30.4 -31.1 -29.4\n",
            "  -34.5 -27.7 -30.3 -30.7 -35.4 -37.4 -36.2 -36.2 -36.2]\n",
            " [-24.  -21.9 -19.9 -21.9 -23.9 -25.9 -27.9 -29.9 -30.  -30.4 -31.1 -32.1\n",
            "  -29.7 -32.3 -32.7 -33.4 -35.4 -37.4 -38.2 -38.2 -38.2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjSX8y_q1nfX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d14c5479-91c4-4e2a-d557-3bb106ea9f6d"
      },
      "source": [
        "print(pi[:,3,5])"
      ],
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LPd4Y887T7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9030fd67-b5d7-423e-ab9e-baefa6b9f1c8"
      },
      "source": [
        "print(vs_new[4,4])\n",
        "print(vs_new[3,5])"
      ],
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-22.62972870053331\n",
            "-24.629728700533285\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}