{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RaceTrack.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM9tmKT3GPgBFpyJK49MUh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandrusuresh/ReinforcementLearning/blob/Racetrack-non-greedy-policy-improvement/Ch5-MonteCarloMethods/RaceTrack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToJb-Y89nKVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "from scipy.special import softmax\n",
        "# np.random.seed(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzPO2dUE6EDS",
        "colab_type": "text"
      },
      "source": [
        "# Race Track\n",
        "\n",
        "This is the solution to Exercise-5.12 in Page-111 of the book [\"Reinforcement Learning\" by Barto, Sutton](http://incompleteideas.net/sutton/book/the-book.html).\n",
        "\n",
        "The summary of the problem is as follows:\n",
        "1. We have a map of a race track that is discretized by a grid of cells. Each cell corresponds to a position on the race track.\n",
        "2. The goal is to find the optimal path from a set of cells marked start line to another set of cells marked the finish line. For simplicity, the start line is assumed to be in the last row of the grid (bottom) and finish line is assumed to be in the last column of the grid (right).\n",
        "3. The track also has a set of valid & invalid cells. The invalid cells represent obstacles or out-of-bounds. No path is allowed to pass through an obstacle.\n",
        "4. Each time a path gets on a cell labelled obstacle, the position resets to a random start cell and the episode continues. The episode ends with the final state vector passes through or gets to any cell labeled finish line.\n",
        "5. The state vector is given by the position and velocity in the x & y directions. i.e. $\\text{state} = [p_x,p_y,v_x,v_y]$ where $p,v$ represent the position and velocity.\n",
        "5. The input vector is the acceleration at each time step given by: $\\text{input} = [a_x,a_y]$ where $a_x,a_y \\in \\{-1,0,1\\}$. The maximum number of possible actions is therefore (=3 $\\times$ 3) 9.\n",
        "6. In the simplest case, the state transition is a deterministic function of the current state and action. The state transition is computed as follows:\n",
        "$$ \\begin{align*} \\left[\\begin{matrix} p_x \\\\p_y \\\\ v_x \\\\ v_y\\end{matrix} \\right]_{k+1} &= \\left[ \\begin{matrix} 1 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 \\end{matrix}\\right] \\left[\\begin{matrix} p_x \\\\p_y \\\\ v_x \\\\ v_y\\end{matrix} \\right]_{k}+ \\left[ \\begin{matrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\\\ 0 & 1 \\end{matrix}\\right] \\left[ \\begin{matrix} a_x \\\\ a_y \\end{matrix}\\right]_k \\\\ \\\\\n",
        "\\Rightarrow s_{k+1} &= A s_k + B a_k \\end{align*}$$\n",
        "\n",
        "##### **Bonus Challenge**\n",
        "8. The above deterministic transition condition is relaxed as follows where the velocity increments (accleration input) are 0 with probability 0.1\n",
        "$$ \\Rightarrow s_{k+1} = \\Bigg\\{ \\begin{matrix} A s_k + B a_k & \\text{with probabilty=0.9} \\\\ A s_k & \\text{with probabilty=0.1} \\end{matrix}$$ \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odvgucmx4eFL",
        "colab_type": "text"
      },
      "source": [
        "### Initialize Tracks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS5tWIN7paU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VALID = 0\n",
        "START = 1\n",
        "PATH = 2\n",
        "FINISH = 3\n",
        "OBSTACLE = 4\n",
        "\n",
        "def get_circuits():\n",
        "  nRows_1 = 32\n",
        "  nCols_1 = 17\n",
        "  map_1 = np.zeros((nRows_1,nCols_1),dtype=np.int8)\n",
        "  map_1[-1,3:9] = START\n",
        "  map_1[0:6,-1] = FINISH\n",
        "  map_1[-3:,:3] = OBSTACLE\n",
        "  map_1[-10:-3:,:2] = OBSTACLE\n",
        "  map_1[-18:-10:,:1] = OBSTACLE\n",
        "  map_1[0,:3] = OBSTACLE\n",
        "  map_1[1:3,:2] = OBSTACLE\n",
        "  map_1[3,:1] = OBSTACLE\n",
        "  map_1[7:,9:] = OBSTACLE\n",
        "  map_1[6,10:] = OBSTACLE\n",
        "\n",
        "  nRows_2 = 30\n",
        "  nCols_2 = 32\n",
        "  map_2 = np.zeros((nRows_2,nCols_2),dtype=np.int8)\n",
        "  map_2[-1,:23] = START\n",
        "  map_2[0:9,-1] = FINISH\n",
        "  map_2[-17:,23:] = OBSTACLE\n",
        "  map_2[-18:,24:] = OBSTACLE\n",
        "  map_2[-19:,26:] = OBSTACLE\n",
        "  map_2[-20:,27:] = OBSTACLE\n",
        "  map_2[-21:,30:] = OBSTACLE\n",
        "  map_2[0,:16] = OBSTACLE\n",
        "  map_2[[1,8],:13] = OBSTACLE\n",
        "  map_2[[2,7],:12] = OBSTACLE\n",
        "  map_2[3:7,:11] = OBSTACLE\n",
        "  map_2[9:14,:14] = OBSTACLE\n",
        "  c = 1\n",
        "  for i in range(14,nRows_2-2):\n",
        "    map_2[i,:14-c] = OBSTACLE\n",
        "    c += 1\n",
        "  return map_1,map_2\n",
        "\n",
        "def plot_circuit(fig,map,num):\n",
        "  nRows,nCols = map.shape\n",
        "  # create discrete colormap\n",
        "  #cmap = colors.ListedColormap(['red', 'blue'])\n",
        "  cmap = colors.ListedColormap(['white','red','blue','green','black'])\n",
        "  bounds = [0,0.1,3]\n",
        "  norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "  ax = fig.add_subplot(1,2,num)\n",
        "  ax.imshow(map, cmap=cmap)#, norm=norm)\n",
        "\n",
        "  ### draw gridlines\n",
        "  # ax = plt.gca();\n",
        "  ax.set_xticks(np.arange(-0.5, nCols+0.5, 1));\n",
        "  ax.set_yticks(np.arange(-0.5, nRows+0.5, 1));\n",
        "  ax.set_xticklabels(np.arange(-0.5, nCols+0.5, 1));\n",
        "  ax.set_yticklabels(np.arange(-0.5, nRows+0.5, 1));\n",
        "  ax.grid(color='k', linestyle='-', linewidth=1)\n",
        "\n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticklabels([])\n",
        "  ax.xaxis.set_ticks_position('none')\n",
        "  ax.yaxis.set_ticks_position('none')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvRmT82s4hvM",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Race Tracks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qyW-kHqz-Va",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "cd4ea036-7fd6-4b2c-f7d3-bcfba35c07e5"
      },
      "source": [
        "map_1,map_2 = get_circuits()\n",
        "fig = plt.figure(figsize=(20,7))#20,7)\n",
        "plot_circuit(fig,map_1,1)\n",
        "plot_circuit(fig,map_2,2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-hLA3pBg74",
        "colab_type": "text"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJ43rrCTBkD6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_velocity = 5\n",
        "actions = np.array([[-1,-1],[-1,0],[-1,1],[0,-1],[0,0],[0,1],[1,-1],[1,0],[1,1]])\n",
        "gamma = 1.0\n",
        "A_mat = np.matrix([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]])\n",
        "B_mat = np.matrix([[1,0],[0,1],[1,0],[0,1]])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7J-6EiFmSPr",
        "colab_type": "text"
      },
      "source": [
        "## Path Finding\n",
        "\n",
        "First, we try to solve the relax the optimal requirement in the problem statement and focus just on being able to find a feasible trajectory from the start to finish line. For this, the Monte-Carlo Off-policy control algorithm in Page-111 of the book is used.\n",
        "1. Behavior policy. The behavior policy is set up to allow exploration of all states and satisfy the problem constraints simultaneously.\n",
        "2. The behavior policy is made restrictive to satisfy the problem constraints to allow more valid control inputs through the MC simulation. This allows a greater fraction of the episodes to terminate and leads to more coverage. In addition, encoding the known constraints in the behavior policy does not hurt exploration in any way. Some of these constraints are encoded in the behavior policy are as follows: \n",
        "\n",
        "*   The path is forbidden to leave the grid from the initial state. This is redundant since the condition for out-of-bounds is checked every step, but is included to speed up convergence\n",
        "*   The velocity components cannot both be zero at any cell other than the start line. This means that any action that makes the speed zero in any valid cell is forbidden by the behavior policy\n",
        "*   The speeds in both directions cannot exceed the max speed limit of 5 units. So the acceleration inputs that increase/decrease the velocith below this threshold are forbidden.\n",
        "\n",
        ">   The behavior policy chooses the actions that satisfy the above constraints with equal probability.\n",
        "\n",
        "3. This part of the problem focusses only on path-finding, therefore the reward is therefore modified to be terminal and all states in the trajectory are given a reward of +1 on reaching the finish line and -1 for going out-of-bounds.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-rplh5P5KOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_state(map):\n",
        "  find_start_line = np.where(map == START)\n",
        "  start_line = np.array(list(zip(find_start_line[0],find_start_line[1])))\n",
        "  choice = np.random.choice(range(start_line.shape[0]))\n",
        "  init = [start_line[choice,0],start_line[choice,1],0,0]\n",
        "  return init\n",
        "\n",
        "def check_termination(map,curr_state):\n",
        "  finish_line = np.where(map == FINISH)\n",
        "  return curr_state[1] >= finish_line[1][0] and curr_state[0] >= np.min(finish_line[0]) and curr_state[0] <= np.max(finish_line[0])\n",
        "\n",
        "def check_OutOfBounds(map,curr_state):\n",
        "  if min(curr_state[0],curr_state[1]) < 0:\n",
        "    return True\n",
        "  elif check_termination(map,curr_state):\n",
        "    return False\n",
        "  elif curr_state[1] >= map.shape[1] or curr_state[0] >= map.shape[0]:\n",
        "    return True\n",
        "  return map[curr_state[0],curr_state[1]] == OBSTACLE\n",
        "\n",
        "def is_init(map,state):\n",
        "  return map[state[0],state[1]] == START\n",
        "\n",
        "def move(state,input):\n",
        "  new_vel = [state[2]+input[0],state[3]+input[1]]\n",
        "  new_state = [state[0]+new_vel[0],state[1]+new_vel[1]] + new_vel\n",
        "  return new_state,new_vel\n",
        "\n",
        "def behavior_policy(map,state):\n",
        "  act_idx = np.arange(actions.shape[0])\n",
        "  prob = 1\n",
        "  if is_init(map,state):\n",
        "    idx = np.where(actions[:,0] < 0)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = idx\n",
        "  # elif state[2]==0 and state[3] == 0:\n",
        "  #   idx = np.arange(actions.shape[0])\n",
        "  #   act_idx = np.delete(idx,[4])\n",
        "  #   prob = 1/float(len(act_idx))\n",
        "  else:\n",
        "    idx = [i for i in range(actions.shape[0]) if state[2]+actions[i,0] == 0 and state[3]+actions[i,1] == 0]\n",
        "    act_idx = np.delete(act_idx,idx)\n",
        "    prob = 1/float(len(act_idx))\n",
        "  if state[2] == max_velocity:\n",
        "    idx = np.where(actions[:,0] < 1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = idx\n",
        "  elif state[2] == -max_velocity:\n",
        "    idx = np.where(actions[:,0] > -1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = idx\n",
        "  if state[3] == max_velocity:\n",
        "    idx = np.where(actions[act_idx,1] < 1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = act_idx[idx]\n",
        "  elif state[3] == -max_velocity:\n",
        "    idx = np.where(actions[act_idx,1] > -1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = act_idx[idx]\n",
        "  if len(act_idx) == actions.shape[0]:\n",
        "    prob = 1/float(actions.shape[0])\n",
        "    act_idx = [np.random.choice(range(actions.shape[0]))]\n",
        "  return actions[np.random.choice(act_idx),:],np.round(prob,3)\n",
        "\n",
        "def get_action_idx(action):\n",
        "  return np.where(np.sum(np.abs(actions-action),1) == 0)[0][0]\n",
        "\n",
        "def play(map,init):\n",
        "  curr_state = np.copy(init)\n",
        "  reward = 0\n",
        "  traj = [curr_state]\n",
        "  action_prob = []\n",
        "  while not check_termination(map,curr_state):\n",
        "    reward -= 1\n",
        "    act,prob = behavior_policy(map,curr_state)\n",
        "    next_state,new_vel = move(curr_state,act)\n",
        "    # print(\"################\")\n",
        "    # print(curr_state)\n",
        "    # print(new_vel)\n",
        "    # print(next_state)\n",
        "    if check_OutOfBounds(map,next_state):\n",
        "      return -1,traj,action_prob\n",
        "      # curr_state = init_state(map)\n",
        "      # traj = [curr_state]\n",
        "      # action_prob = []\n",
        "      # continue\n",
        "    else:\n",
        "      curr_state = next_state\n",
        "    action_prob += [list(act) + [prob]]\n",
        "    traj += [curr_state]\n",
        "  return 1,traj,action_prob\n",
        "\n",
        "def target_policy(Q,state):\n",
        "  best_action = [np.argmax(Q[state[0],state[1],max_velocity+state[2],max_velocity+state[3],:])]\n",
        "  if len(best_action) < 1:\n",
        "    print('No Best Action')\n",
        "    print('State:',state)\n",
        "    print('Action:',Q[state[0],state[1],:])\n",
        "  return np.random.choice(best_action),np.round(1/float(len(best_action)),3)\n",
        "\n",
        "def play_target(map,Q,init):\n",
        "  curr_state = np.copy(init)\n",
        "  reward = 0\n",
        "  traj = [curr_state]\n",
        "  action_prob = []\n",
        "  while not check_termination(map,curr_state):\n",
        "    reward -= 1\n",
        "    a,prob = target_policy(Q,curr_state)\n",
        "    act = actions[a]\n",
        "    new_vel = [curr_state[2]+act[0],curr_state[3]+act[1]]\n",
        "    next_state = [curr_state[0]+new_vel[0],curr_state[1]+new_vel[1],new_vel[0],new_vel[1]] \n",
        "    if check_OutOfBounds(map,next_state):\n",
        "      # curr_state = init_state(map)\n",
        "      break\n",
        "    else:\n",
        "      curr_state = next_state\n",
        "    action_prob += [list(act) + [prob]]\n",
        "    traj += [curr_state]\n",
        "  return reward,traj,action_prob\n",
        "\n",
        "def MonteCarlo_OffPolicyControl(map,num_episodes):\n",
        "  nRows,nCols = map.shape\n",
        "  action_values  = np.zeros((nRows,nCols,2*max_velocity+1,2*max_velocity+1,len(actions)))\n",
        "  action_weights = np.zeros((nRows,nCols,2*max_velocity+1,2*max_velocity+1,len(actions)))\n",
        "\n",
        "  for episode in tqdm(range(num_episodes)):\n",
        "    init = init_state(map)\n",
        "    reward,traj,action_prob = play(map,init)\n",
        "    G = 0\n",
        "    W = 1\n",
        "    for i in range(len(traj)-2,-1,-1):\n",
        "      t = traj[i]\n",
        "      act_prob = action_prob[i]\n",
        "      G = reward#gamma*G + reward\n",
        "      act_idx = get_action_idx(np.array(act_prob[:2]))\n",
        "      action_weights[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx] += W\n",
        "      action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx] += W/action_weights[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx]*(G-action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx])\n",
        "      pi_t,_ = target_policy(action_values,t)\n",
        "      if pi_t != act_idx:\n",
        "        break\n",
        "      W = W/act_prob[-1]\n",
        "  return action_values\n",
        "\n",
        "def generate_path(map,Q):\n",
        "  final_map = []\n",
        "  for k in range(2):\n",
        "    init = init_state(map)\n",
        "    reward,traj,action_prob = play_target(map,Q,init)\n",
        "    print('Reward = :',reward)\n",
        "    final_map.append(np.copy(map))\n",
        "    for i,t in enumerate(traj):\n",
        "      final_map[-1][min(t[0],map.shape[0]-1),min(t[1],map.shape[1]-1)] = PATH\n",
        "  fig = plt.figure(figsize=(20,7))\n",
        "  plot_circuit(fig,final_map[0],1)\n",
        "  plot_circuit(fig,final_map[1],2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoMxzO-Ar5Qb",
        "colab_type": "text"
      },
      "source": [
        "## Monte-Carlo Off-Policy Control"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym8vTXuD4Yta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8c0b32bc-c922-479c-ad32-939c0ebddd5d"
      },
      "source": [
        "num_episodes = 100000\n",
        "\n",
        "Q1 = MonteCarlo_OffPolicyControl(map_1,num_episodes)\n",
        "\n",
        "Q2 = MonteCarlo_OffPolicyControl(map_2,num_episodes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:59<00:00, 1686.13it/s]\n",
            "100%|██████████| 100000/100000 [01:32<00:00, 1085.59it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWxqZOLa1mPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "d6d75c79-9865-4cf5-ed7b-1470b4ef1d77"
      },
      "source": [
        "generate_path(map_1,Q1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward = : -13\n",
            "Reward = : -17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLJxeHhhvavI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "8d4be7d4-24ef-4075-be65-e83e22bcf589"
      },
      "source": [
        "generate_path(map_2,Q2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward = : -15\n",
            "Reward = : -15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceeeQyNxyPrR",
        "colab_type": "text"
      },
      "source": [
        "### Non-greedy target policy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fw6NkAKwybbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_target_policy(Q,state):\n",
        "  idx = np.where(Q[state[0],state[1],max_velocity+state[2],max_velocity+state[3],:] != 0)[0]\n",
        "  if len(idx) > 0:\n",
        "    # print(idx)\n",
        "    # print(np.argmax(Q[state[0],state[1],max_velocity+state[2],max_velocity+state[3],idx]))\n",
        "    best_action = [idx[np.argmax(Q[state[0],state[1],max_velocity+state[2],max_velocity+state[3],idx])]]\n",
        "  else:\n",
        "    best_action = np.arange(actions.shape[0])\n",
        "  return actions[np.random.choice(best_action),:],np.round(1/float(len(best_action)),3)\n",
        "\n",
        "def init_random(map):\n",
        "  find_valid = np.where(map <= START)\n",
        "  choice = np.random.choice(range(len(find_valid[0])))\n",
        "  init = [find_valid[0][choice],find_valid[1][choice],0,0]\n",
        "  num = (2*max_velocity+1)\n",
        "  num_sq = num**2\n",
        "  prob = 1/float(num_sq)*np.ones((num_sq,))\n",
        "  if is_init(map,init):\n",
        "    return init\n",
        "  prob = 1/float(num_sq-1)*np.ones((num_sq,))\n",
        "  i = max_velocity\n",
        "  j = max_velocity\n",
        "  prob[i*(2*max_velocity+1)+j] = 0\n",
        "  idx = np.random.choice(range(num_sq),p=prob)\n",
        "  init[2] = idx//num - max_velocity\n",
        "  init[3] = idx%num - max_velocity\n",
        "  return init\n",
        "\n",
        "def softmax_target_policy(map,state,Q):\n",
        "  act_idx = np.arange(actions.shape[0])\n",
        "  prob = 1\n",
        "  if is_init(map,state):\n",
        "    idx = np.where(actions[:,0] < 0)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = idx\n",
        "  else:\n",
        "    idx = [i for i in range(actions.shape[0]) if state[2]+actions[i,0] == 0 and state[3]+actions[i,1] == 0]\n",
        "    act_idx = np.delete(act_idx,idx)\n",
        "    prob = 1/float(len(act_idx))\n",
        "  if state[2] == max_velocity:\n",
        "    idx = np.where(actions[:,0] < 1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = idx\n",
        "  elif state[2] == -max_velocity:\n",
        "    idx = np.where(actions[:,0] > -1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = idx\n",
        "  if state[3] == max_velocity:\n",
        "    idx = np.where(actions[act_idx,1] < 1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = act_idx[idx]\n",
        "  elif state[3] == -max_velocity:\n",
        "    idx = np.where(actions[act_idx,1] > -1)[0]\n",
        "    prob = 1/float(len(idx))\n",
        "    act_idx = act_idx[idx]\n",
        "  if len(act_idx) == actions.shape[0]:\n",
        "    prob = 1/float(actions.shape[0])\n",
        "    act_idx = [np.random.choice(range(actions.shape[0]))]\n",
        "  if len(act_idx) < 1:\n",
        "    print('No possible actions:',state)\n",
        "  allowed_Q = Q[state[0],state[1],state[2],state[3],act_idx]\n",
        "  prob = softmax(allowed_Q)\n",
        "  best_action = np.random.choice(act_idx,p=prob)\n",
        "  # print('All Actions:',act_idx)\n",
        "  # print('Best:',best_action)\n",
        "  # print('Prob:',prob)\n",
        "  # print(prob)\n",
        "  return actions[best_action,:],np.round(prob[list(act_idx).index(best_action)],3)\n",
        "\n",
        "def play_nongreedy_target(map,Q,init):\n",
        "  curr_state = np.copy(init)\n",
        "  reward = 0\n",
        "  traj = [curr_state]\n",
        "  action_prob = []\n",
        "  while not check_termination(map,curr_state):\n",
        "    reward -= 1\n",
        "    act,prob = softmax_target_policy(map,curr_state,Q)\n",
        "    next_state,new_vel = move(curr_state,act)\n",
        "    if check_OutOfBounds(map,next_state):\n",
        "      return -1,traj,action_prob\n",
        "    else:\n",
        "      curr_state = next_state\n",
        "    action_prob += [list(act) + [prob]]\n",
        "    traj += [curr_state]\n",
        "  return 1,traj,action_prob\n",
        "\n",
        "def play_greedy_target(map,Q,init):\n",
        "  curr_state = np.copy(init)\n",
        "  reward = 0\n",
        "  traj = [curr_state]\n",
        "  action_prob = []\n",
        "  while not check_termination(map,curr_state):\n",
        "    reward -= 1\n",
        "    act,prob = greedy_target_policy(Q,curr_state)\n",
        "    next_state,new_vel = move(curr_state,act)\n",
        "    if check_OutOfBounds(map,next_state):\n",
        "      return -1,traj,action_prob\n",
        "    else:\n",
        "      curr_state = next_state\n",
        "    action_prob += [list(act) + [prob]]\n",
        "    traj += [curr_state]\n",
        "  return 1,traj,action_prob\n",
        "\n",
        "def generate_greedy_path(map,Q):\n",
        "  final_map = []\n",
        "  for k in range(2):\n",
        "    init = init_state(map)\n",
        "    reward,traj,action_prob = play_greedy_target(map,Q,init)\n",
        "    print('Reward = :',reward)\n",
        "    final_map.append(np.copy(map))\n",
        "    for i,t in enumerate(traj):\n",
        "      final_map[-1][min(t[0],map.shape[0]-1),min(t[1],map.shape[1]-1)] = PATH\n",
        "  fig = plt.figure(figsize=(20,7))\n",
        "  plot_circuit(fig,final_map[0],1)\n",
        "  plot_circuit(fig,final_map[1],2)\n",
        "\n",
        "def MonteCarlo_OffPolicyControl_nongreedy(map,num_episodes,Q):\n",
        "  nRows,nCols = map.shape\n",
        "  min_act_val = 0;\n",
        "  action_values  = min_act_val*np.ones((nRows,nCols,2*max_velocity+1,2*max_velocity+1,len(actions)))\n",
        "  action_weights = np.zeros((nRows,nCols,2*max_velocity+1,2*max_velocity+1,len(actions)))\n",
        "\n",
        "  for episode in tqdm(range(num_episodes)):\n",
        "    # init = init_state(map)\n",
        "    init = init_random(map)\n",
        "    reward,traj,action_prob = play_nongreedy_target(map,Q,init)\n",
        "    G = 0\n",
        "    W = 1\n",
        "    for i in range(len(traj)-2,-1,-1):\n",
        "      t = traj[i]\n",
        "      act_prob = action_prob[i]\n",
        "      G = reward#gamma*G + reward\n",
        "      act_idx = get_action_idx(np.array(act_prob[:2]))\n",
        "      action_weights[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx] += W\n",
        "      if action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx] == min_act_val:\n",
        "        action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx] = W/action_weights[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx]*(G-action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx])\n",
        "      else:\n",
        "        action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx] += W/action_weights[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx]*(G-action_values[t[0],t[1],max_velocity+t[2],max_velocity+t[3],act_idx])\n",
        "      pi_t,_ = target_policy(action_values,t)\n",
        "      if pi_t != act_idx:\n",
        "        break\n",
        "      W = W/act_prob[-1]\n",
        "  return action_values"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDVejWh113Ry",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bb98253d-a7b1-436b-c5d6-140ca7969501"
      },
      "source": [
        "num_episodes = 1000000\n",
        "Q1_nongreedy = MonteCarlo_OffPolicyControl_nongreedy(map_1,num_episodes,Q1)\n",
        "\n",
        "# # Q2_nongreedy = MonteCarlo_OffPolicyControl(map_2,num_episodes,Q2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000000/1000000 [13:26<00:00, 1239.66it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts32-SDVzJM5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9c1cd2d-8360-40a1-bc44-ac61183e094b"
      },
      "source": [
        "generate_greedy_path(map_1,Q1_nongreedy)\n",
        "# generate_path(map_1,Q1_nongreedy)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2]\n",
            "0\n",
            "[0 3]\n",
            "0\n",
            "[1 4 7]\n",
            "0\n",
            "[1 4 7]\n",
            "0\n",
            "[1 4 7]\n",
            "0\n",
            "[4 7]\n",
            "0\n",
            "[7]\n",
            "0\n",
            "[0 1 2 3 4 5 6 7 8]\n",
            "0\n",
            "Reward = : -1\n",
            "[1]\n",
            "0\n",
            "[0 1 4]\n",
            "1\n",
            "[0 1 3 6]\n",
            "1\n",
            "[0 3 4 6]\n",
            "2\n",
            "[0 3 4 6]\n",
            "2\n",
            "[3 5 6]\n",
            "1\n",
            "[2 5 8]\n",
            "0\n",
            "[4 6 7]\n",
            "0\n",
            "[0 1 2 3 4 5 8]\n",
            "6\n",
            "[0 1 2 3 4 5 6 7 8]\n",
            "6\n",
            "[0 1 2 3 4 5 6 8]\n",
            "2\n",
            "Reward = : 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM3bOm-j0jtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate_nongreedy_path(map_2,Q2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO56c38-8i30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = 16\n",
        "y = 8\n",
        "Q1_test = Q1_nongreedy[x,y,:,:,:]\n",
        "for i in range(Q1_test.shape[0]):\n",
        "  for j in range(Q1_test.shape[1]):\n",
        "    print(\"################\")\n",
        "    print('state = ',[x,y,i-max_velocity,j-max_velocity])\n",
        "    print(Q1_test[i,j,:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuv2vJq9CKNC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "ef158e99-a6cb-477e-dc6a-f4e5c42ce926"
      },
      "source": [
        "for i in range(10):\n",
        "  print(init_random(map_1))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[16, 1, -5, 0]\n",
            "[0, 6, 4, 2]\n",
            "[31, 4, 0, 0]\n",
            "[0, 5, -3, -1]\n",
            "[30, 4, -5, -1]\n",
            "[7, 4, 4, 1]\n",
            "[17, 4, -2, -3]\n",
            "[6, 0, -1, -4]\n",
            "[5, 1, 2, 1]\n",
            "[8, 5, -3, -3]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}