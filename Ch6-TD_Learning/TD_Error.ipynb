{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD_Error.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNEVN4aMWiYEPIJP1EodPYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandrusuresh/ReinforcementLearning/blob/master/Ch6-TD_Learning/TD_Error.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H9SBBx1cuC6",
        "colab_type": "text"
      },
      "source": [
        "## Solution to Exercise 6.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDHtW7BvDWDx",
        "colab_type": "text"
      },
      "source": [
        "## Monte-Carlo Error\n",
        "\n",
        "This is the error in the estimate at each episode when the values are updated at the end of the episode after the reward is known.\n",
        "\n",
        "The Monte-Carlo Error is given by $G_t - V(S_t)$ which simplifies to:\n",
        "$$ \\begin{align*} G_t - V(S_t) &= R_{t+1} + \\gamma G_{t+1} - V(S_t)\\\\\n",
        "&= R_{t+1} + \\gamma G_{t+1} - V(S_t) + \\gamma V(S_{t+1}) -  \\gamma V(S_{t+1}) \\end{align*}$$\n",
        "\n",
        "Given, $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$\n",
        "\n",
        "$$ \\begin{align*} G_t - V(S_t) &= \\delta_t + \\gamma (G_{t+1} - V(S_{t+1})) \\\\ \n",
        "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 (G_{t+2} - V(S_{t+2})) \\\\\n",
        "&= \\delta_t + \\gamma \\delta_{t+1} + \\ldots + \\gamma^{T-t} (G_T - V(S_T)) \\end{align*}$$\n",
        "\n",
        "Since $G_T = V(S_T)$, we have the final simplified term for MC error as:\n",
        "\n",
        "$$ G_t - V(S_t) = \\sum_{k=t}^{T} \\gamma^{k-t} \\delta_k $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wuryBFcFiRc",
        "colab_type": "text"
      },
      "source": [
        "## Bootstrap Error\n",
        "This is the error in the estimate at each episode when the values are updated at each step of the episode before the (final/terminal) reward is known.\n",
        "\n",
        "The update at iteration $k$ is given as:\n",
        "$ V_{k}(S_t) = V_{k-1}(S_t) + \\alpha (R_{k,t+1} + \\gamma V_{k-1}(S_{t+1}) - V_{k-1}(S_t))$\n",
        "\n",
        "The Bootstrap Error is given by $G_t - V(S_t)$ which simplifies to:\n",
        "$$ \\begin{align*} G_{k,t} - V_{k}(S_t) &= R_{k,t+1} + \\gamma G_{k,t+1} - V_k(S_t)\\\\\n",
        "&= R_{k,t+1} + \\gamma G_{k,t+1} - V_{k-1}(S_t) - \\alpha (R_{k,t+1} + \\gamma V_{k-1}(S_{t+1}) - V_{k-1}(S_t)) \\\\\n",
        "&= R_{k,t+1} + \\gamma G_{k,t+1} - V_{k-1}(S_t) - \\alpha (R_{k,t+1} + \\gamma V_{k-1}(S_{t+1}) - V_{k-1}(S_t)) + \\gamma V_{k-1}(S_{t+1}) -  \\gamma V_{k-1}(S_{t+1}) \\\\\n",
        "&= (1-\\alpha)R_{k,t+1} - (1-\\alpha)V_{k-1}(S_t) + \\gamma (G_{k,t+1} - V_{k-1}(S_{t+1})) + (1-\\alpha) \\gamma V_{k-1}(S_{t+1})  + \\gamma V_{k}(S_{t+1}) -  \\gamma V_{k}(S_{t+1})\\\\\n",
        "&= (1-\\alpha)\\delta_t + \\gamma (G_{k,t+1} - V_{k}(S_{t+1})) + \\gamma (V_{k}(S_{t+1}) - V_{k-1}(S_{t+1}))\\\\ \n",
        "&= (1-\\alpha)\\delta_t + \\gamma ((1-\\alpha)\\delta_{t+1} + \\gamma (G_{k,t+2} - V_{k}(S_{t+2})) + \\gamma \\alpha \\delta_{t+2}) + \\gamma \\alpha \\delta_{t+1}\\\\\n",
        "&= (1-\\alpha) \\delta_t + \\gamma \\delta_{t+1} + \\ldots + \\gamma^{T-t} (G_T - V(S_T))\\\\\n",
        "&= \\sum_{i=t}^{T} \\gamma^{i-t} \\delta_i - \\alpha \\delta_t \\end{align*}$$\n"
      ]
    }
  ]
}